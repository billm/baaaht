// llm.proto - gRPC service definitions for LLM Gateway communication
//
// This file defines the LLMService which provides streaming LLM request handling,
// token tracking, and provider abstraction for agent communication.
//
// Copyright 2026 baaaht project

// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.11
// 	protoc        v6.33.4
// source: llm.proto

package proto

import (
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	durationpb "google.golang.org/protobuf/types/known/durationpb"
	emptypb "google.golang.org/protobuf/types/known/emptypb"
	timestamppb "google.golang.org/protobuf/types/known/timestamppb"
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

// FinishReason represents the reason generation stopped
type FinishReason int32

const (
	FinishReason_FINISH_REASON_UNSPECIFIED FinishReason = 0
	FinishReason_FINISH_REASON_STOP        FinishReason = 1 // Natural stop
	FinishReason_FINISH_REASON_LENGTH      FinishReason = 2 // Max tokens reached
	FinishReason_FINISH_REASON_TOOL_USES   FinishReason = 3 // Tool use requested
	FinishReason_FINISH_REASON_ERROR       FinishReason = 4 // Error occurred
	FinishReason_FINISH_REASON_FILTER      FinishReason = 5 // Content filtered
)

// Enum value maps for FinishReason.
var (
	FinishReason_name = map[int32]string{
		0: "FINISH_REASON_UNSPECIFIED",
		1: "FINISH_REASON_STOP",
		2: "FINISH_REASON_LENGTH",
		3: "FINISH_REASON_TOOL_USES",
		4: "FINISH_REASON_ERROR",
		5: "FINISH_REASON_FILTER",
	}
	FinishReason_value = map[string]int32{
		"FINISH_REASON_UNSPECIFIED": 0,
		"FINISH_REASON_STOP":        1,
		"FINISH_REASON_LENGTH":      2,
		"FINISH_REASON_TOOL_USES":   3,
		"FINISH_REASON_ERROR":       4,
		"FINISH_REASON_FILTER":      5,
	}
)

func (x FinishReason) Enum() *FinishReason {
	p := new(FinishReason)
	*p = x
	return p
}

func (x FinishReason) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (FinishReason) Descriptor() protoreflect.EnumDescriptor {
	return file_llm_proto_enumTypes[0].Descriptor()
}

func (FinishReason) Type() protoreflect.EnumType {
	return &file_llm_proto_enumTypes[0]
}

func (x FinishReason) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use FinishReason.Descriptor instead.
func (FinishReason) EnumDescriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{0}
}

// LLMRequest represents a request to an LLM provider
type LLMRequest struct {
	state             protoimpl.MessageState `protogen:"open.v1"`
	RequestId         string                 `protobuf:"bytes,1,opt,name=request_id,json=requestId,proto3" json:"request_id,omitempty"`                         // Unique identifier for this request
	SessionId         string                 `protobuf:"bytes,2,opt,name=session_id,json=sessionId,proto3" json:"session_id,omitempty"`                         // Session identifier for tracking
	Model             string                 `protobuf:"bytes,3,opt,name=model,proto3" json:"model,omitempty"`                                                  // Model identifier (e.g., "anthropic/claude-sonnet-4-20250514")
	Messages          []*LLMMessage          `protobuf:"bytes,4,rep,name=messages,proto3" json:"messages,omitempty"`                                            // Conversation messages
	Parameters        *LLMParameters         `protobuf:"bytes,5,opt,name=parameters,proto3" json:"parameters,omitempty"`                                        // Generation parameters
	Tools             []*Tool                `protobuf:"bytes,6,rep,name=tools,proto3" json:"tools,omitempty"`                                                  // Available tools for the LLM
	Provider          string                 `protobuf:"bytes,7,opt,name=provider,proto3" json:"provider,omitempty"`                                            // Provider override (optional)
	FallbackProviders []string               `protobuf:"bytes,8,rep,name=fallback_providers,json=fallbackProviders,proto3" json:"fallback_providers,omitempty"` // Fallback provider chain (optional)
	Metadata          *LLMMetadata           `protobuf:"bytes,9,opt,name=metadata,proto3" json:"metadata,omitempty"`                                            // Additional metadata
	unknownFields     protoimpl.UnknownFields
	sizeCache         protoimpl.SizeCache
}

func (x *LLMRequest) Reset() {
	*x = LLMRequest{}
	mi := &file_llm_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *LLMRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*LLMRequest) ProtoMessage() {}

func (x *LLMRequest) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use LLMRequest.ProtoReflect.Descriptor instead.
func (*LLMRequest) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{0}
}

func (x *LLMRequest) GetRequestId() string {
	if x != nil {
		return x.RequestId
	}
	return ""
}

func (x *LLMRequest) GetSessionId() string {
	if x != nil {
		return x.SessionId
	}
	return ""
}

func (x *LLMRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *LLMRequest) GetMessages() []*LLMMessage {
	if x != nil {
		return x.Messages
	}
	return nil
}

func (x *LLMRequest) GetParameters() *LLMParameters {
	if x != nil {
		return x.Parameters
	}
	return nil
}

func (x *LLMRequest) GetTools() []*Tool {
	if x != nil {
		return x.Tools
	}
	return nil
}

func (x *LLMRequest) GetProvider() string {
	if x != nil {
		return x.Provider
	}
	return ""
}

func (x *LLMRequest) GetFallbackProviders() []string {
	if x != nil {
		return x.FallbackProviders
	}
	return nil
}

func (x *LLMRequest) GetMetadata() *LLMMetadata {
	if x != nil {
		return x.Metadata
	}
	return nil
}

// LLMMessage represents a message in the conversation
type LLMMessage struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Role          string                 `protobuf:"bytes,1,opt,name=role,proto3" json:"role,omitempty"`                                                                             // Role: "user", "assistant", "system"
	Content       string                 `protobuf:"bytes,2,opt,name=content,proto3" json:"content,omitempty"`                                                                       // Message content
	Extra         map[string]string      `protobuf:"bytes,3,rep,name=extra,proto3" json:"extra,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"` // Additional fields (e.g., cache_control)
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *LLMMessage) Reset() {
	*x = LLMMessage{}
	mi := &file_llm_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *LLMMessage) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*LLMMessage) ProtoMessage() {}

func (x *LLMMessage) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use LLMMessage.ProtoReflect.Descriptor instead.
func (*LLMMessage) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{1}
}

func (x *LLMMessage) GetRole() string {
	if x != nil {
		return x.Role
	}
	return ""
}

func (x *LLMMessage) GetContent() string {
	if x != nil {
		return x.Content
	}
	return ""
}

func (x *LLMMessage) GetExtra() map[string]string {
	if x != nil {
		return x.Extra
	}
	return nil
}

// LLMParameters contains generation parameters for the LLM
type LLMParameters struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	MaxTokens     int32                  `protobuf:"varint,1,opt,name=max_tokens,json=maxTokens,proto3" json:"max_tokens,omitempty"`            // Maximum tokens to generate
	Temperature   float64                `protobuf:"fixed64,2,opt,name=temperature,proto3" json:"temperature,omitempty"`                        // Sampling temperature (0.0 to 1.0)
	TopP          float64                `protobuf:"fixed64,3,opt,name=top_p,json=topP,proto3" json:"top_p,omitempty"`                          // Nucleus sampling threshold
	TopK          int32                  `protobuf:"varint,4,opt,name=top_k,json=topK,proto3" json:"top_k,omitempty"`                           // Top-k sampling parameter
	StopSequences []string               `protobuf:"bytes,5,rep,name=stop_sequences,json=stopSequences,proto3" json:"stop_sequences,omitempty"` // Sequences that stop generation
	Stream        bool                   `protobuf:"varint,6,opt,name=stream,proto3" json:"stream,omitempty"`                                   // Whether to stream responses
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *LLMParameters) Reset() {
	*x = LLMParameters{}
	mi := &file_llm_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *LLMParameters) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*LLMParameters) ProtoMessage() {}

func (x *LLMParameters) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use LLMParameters.ProtoReflect.Descriptor instead.
func (*LLMParameters) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{2}
}

func (x *LLMParameters) GetMaxTokens() int32 {
	if x != nil {
		return x.MaxTokens
	}
	return 0
}

func (x *LLMParameters) GetTemperature() float64 {
	if x != nil {
		return x.Temperature
	}
	return 0
}

func (x *LLMParameters) GetTopP() float64 {
	if x != nil {
		return x.TopP
	}
	return 0
}

func (x *LLMParameters) GetTopK() int32 {
	if x != nil {
		return x.TopK
	}
	return 0
}

func (x *LLMParameters) GetStopSequences() []string {
	if x != nil {
		return x.StopSequences
	}
	return nil
}

func (x *LLMParameters) GetStream() bool {
	if x != nil {
		return x.Stream
	}
	return false
}

// Tool represents a tool/function available to the LLM
type Tool struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Name          string                 `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`                                  // Tool name
	Description   string                 `protobuf:"bytes,2,opt,name=description,proto3" json:"description,omitempty"`                    // Tool description
	InputSchema   *ToolInputSchema       `protobuf:"bytes,3,opt,name=input_schema,json=inputSchema,proto3" json:"input_schema,omitempty"` // JSON schema for tool input
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Tool) Reset() {
	*x = Tool{}
	mi := &file_llm_proto_msgTypes[3]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Tool) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Tool) ProtoMessage() {}

func (x *Tool) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[3]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Tool.ProtoReflect.Descriptor instead.
func (*Tool) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{3}
}

func (x *Tool) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *Tool) GetDescription() string {
	if x != nil {
		return x.Description
	}
	return ""
}

func (x *Tool) GetInputSchema() *ToolInputSchema {
	if x != nil {
		return x.InputSchema
	}
	return nil
}

// ToolInputSchema represents the JSON schema for tool input
type ToolInputSchema struct {
	state         protoimpl.MessageState   `protogen:"open.v1"`
	Type          string                   `protobuf:"bytes,1,opt,name=type,proto3" json:"type,omitempty"`                                                                                       // JSON Schema type (e.g., "object")
	Properties    map[string]*ToolProperty `protobuf:"bytes,2,rep,name=properties,proto3" json:"properties,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"` // Input properties
	Required      []string                 `protobuf:"bytes,3,rep,name=required,proto3" json:"required,omitempty"`                                                                               // Required properties
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ToolInputSchema) Reset() {
	*x = ToolInputSchema{}
	mi := &file_llm_proto_msgTypes[4]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ToolInputSchema) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ToolInputSchema) ProtoMessage() {}

func (x *ToolInputSchema) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[4]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ToolInputSchema.ProtoReflect.Descriptor instead.
func (*ToolInputSchema) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{4}
}

func (x *ToolInputSchema) GetType() string {
	if x != nil {
		return x.Type
	}
	return ""
}

func (x *ToolInputSchema) GetProperties() map[string]*ToolProperty {
	if x != nil {
		return x.Properties
	}
	return nil
}

func (x *ToolInputSchema) GetRequired() []string {
	if x != nil {
		return x.Required
	}
	return nil
}

// ToolProperty represents a property in the tool input schema
type ToolProperty struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Type          string                 `protobuf:"bytes,1,opt,name=type,proto3" json:"type,omitempty"`               // Property type
	Description   string                 `protobuf:"bytes,2,opt,name=description,proto3" json:"description,omitempty"` // Property description
	Enum          []string               `protobuf:"bytes,3,rep,name=enum,proto3" json:"enum,omitempty"`               // Enum values (if applicable)
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ToolProperty) Reset() {
	*x = ToolProperty{}
	mi := &file_llm_proto_msgTypes[5]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ToolProperty) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ToolProperty) ProtoMessage() {}

func (x *ToolProperty) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[5]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ToolProperty.ProtoReflect.Descriptor instead.
func (*ToolProperty) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{5}
}

func (x *ToolProperty) GetType() string {
	if x != nil {
		return x.Type
	}
	return ""
}

func (x *ToolProperty) GetDescription() string {
	if x != nil {
		return x.Description
	}
	return ""
}

func (x *ToolProperty) GetEnum() []string {
	if x != nil {
		return x.Enum
	}
	return nil
}

// LLMMetadata contains additional metadata for LLM requests
type LLMMetadata struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	CreatedAt     *timestamppb.Timestamp `protobuf:"bytes,1,opt,name=created_at,json=createdAt,proto3" json:"created_at,omitempty"`
	AgentId       string                 `protobuf:"bytes,2,opt,name=agent_id,json=agentId,proto3" json:"agent_id,omitempty"`                                                          // Optional: Agent making the request
	ContainerId   string                 `protobuf:"bytes,3,opt,name=container_id,json=containerId,proto3" json:"container_id,omitempty"`                                              // Optional: Container making the request
	Labels        map[string]string      `protobuf:"bytes,4,rep,name=labels,proto3" json:"labels,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"` // Optional labels for tracking
	TimeoutMs     int64                  `protobuf:"varint,5,opt,name=timeout_ms,json=timeoutMs,proto3" json:"timeout_ms,omitempty"`                                                   // Request timeout in milliseconds
	CorrelationId string                 `protobuf:"bytes,6,opt,name=correlation_id,json=correlationId,proto3" json:"correlation_id,omitempty"`                                        // For correlating requests/responses
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *LLMMetadata) Reset() {
	*x = LLMMetadata{}
	mi := &file_llm_proto_msgTypes[6]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *LLMMetadata) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*LLMMetadata) ProtoMessage() {}

func (x *LLMMetadata) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[6]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use LLMMetadata.ProtoReflect.Descriptor instead.
func (*LLMMetadata) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{6}
}

func (x *LLMMetadata) GetCreatedAt() *timestamppb.Timestamp {
	if x != nil {
		return x.CreatedAt
	}
	return nil
}

func (x *LLMMetadata) GetAgentId() string {
	if x != nil {
		return x.AgentId
	}
	return ""
}

func (x *LLMMetadata) GetContainerId() string {
	if x != nil {
		return x.ContainerId
	}
	return ""
}

func (x *LLMMetadata) GetLabels() map[string]string {
	if x != nil {
		return x.Labels
	}
	return nil
}

func (x *LLMMetadata) GetTimeoutMs() int64 {
	if x != nil {
		return x.TimeoutMs
	}
	return 0
}

func (x *LLMMetadata) GetCorrelationId() string {
	if x != nil {
		return x.CorrelationId
	}
	return ""
}

// LLMResponse represents a response from an LLM provider
type LLMResponse struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	RequestId     string                 `protobuf:"bytes,1,opt,name=request_id,json=requestId,proto3" json:"request_id,omitempty"`                                    // Correlates with the request
	Content       string                 `protobuf:"bytes,2,opt,name=content,proto3" json:"content,omitempty"`                                                         // Response content
	ToolCalls     []*ToolCall            `protobuf:"bytes,3,rep,name=tool_calls,json=toolCalls,proto3" json:"tool_calls,omitempty"`                                    // Tool calls made by the LLM
	Usage         *TokenUsage            `protobuf:"bytes,4,opt,name=usage,proto3" json:"usage,omitempty"`                                                             // Token usage information
	FinishReason  FinishReason           `protobuf:"varint,5,opt,name=finish_reason,json=finishReason,proto3,enum=llm.v1.FinishReason" json:"finish_reason,omitempty"` // Reason generation stopped
	Provider      string                 `protobuf:"bytes,6,opt,name=provider,proto3" json:"provider,omitempty"`                                                       // Provider that handled the request
	Model         string                 `protobuf:"bytes,7,opt,name=model,proto3" json:"model,omitempty"`                                                             // Model that handled the request
	CompletedAt   *timestamppb.Timestamp `protobuf:"bytes,8,opt,name=completed_at,json=completedAt,proto3" json:"completed_at,omitempty"`
	Metadata      *LLMMetadata           `protobuf:"bytes,9,opt,name=metadata,proto3" json:"metadata,omitempty"` // Request metadata echoed back
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *LLMResponse) Reset() {
	*x = LLMResponse{}
	mi := &file_llm_proto_msgTypes[7]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *LLMResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*LLMResponse) ProtoMessage() {}

func (x *LLMResponse) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[7]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use LLMResponse.ProtoReflect.Descriptor instead.
func (*LLMResponse) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{7}
}

func (x *LLMResponse) GetRequestId() string {
	if x != nil {
		return x.RequestId
	}
	return ""
}

func (x *LLMResponse) GetContent() string {
	if x != nil {
		return x.Content
	}
	return ""
}

func (x *LLMResponse) GetToolCalls() []*ToolCall {
	if x != nil {
		return x.ToolCalls
	}
	return nil
}

func (x *LLMResponse) GetUsage() *TokenUsage {
	if x != nil {
		return x.Usage
	}
	return nil
}

func (x *LLMResponse) GetFinishReason() FinishReason {
	if x != nil {
		return x.FinishReason
	}
	return FinishReason_FINISH_REASON_UNSPECIFIED
}

func (x *LLMResponse) GetProvider() string {
	if x != nil {
		return x.Provider
	}
	return ""
}

func (x *LLMResponse) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *LLMResponse) GetCompletedAt() *timestamppb.Timestamp {
	if x != nil {
		return x.CompletedAt
	}
	return nil
}

func (x *LLMResponse) GetMetadata() *LLMMetadata {
	if x != nil {
		return x.Metadata
	}
	return nil
}

// ToolCall represents a tool call made by the LLM
type ToolCall struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Id            string                 `protobuf:"bytes,1,opt,name=id,proto3" json:"id,omitempty"`               // Tool call ID
	Name          string                 `protobuf:"bytes,2,opt,name=name,proto3" json:"name,omitempty"`           // Tool/function name
	Arguments     string                 `protobuf:"bytes,3,opt,name=arguments,proto3" json:"arguments,omitempty"` // JSON string of arguments
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ToolCall) Reset() {
	*x = ToolCall{}
	mi := &file_llm_proto_msgTypes[8]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ToolCall) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ToolCall) ProtoMessage() {}

func (x *ToolCall) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[8]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ToolCall.ProtoReflect.Descriptor instead.
func (*ToolCall) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{8}
}

func (x *ToolCall) GetId() string {
	if x != nil {
		return x.Id
	}
	return ""
}

func (x *ToolCall) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *ToolCall) GetArguments() string {
	if x != nil {
		return x.Arguments
	}
	return ""
}

// StreamLLMRequest is a request in the streaming LLM flow
type StreamLLMRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Types that are valid to be assigned to Payload:
	//
	//	*StreamLLMRequest_Request
	//	*StreamLLMRequest_Heartbeat
	Payload       isStreamLLMRequest_Payload `protobuf_oneof:"payload"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *StreamLLMRequest) Reset() {
	*x = StreamLLMRequest{}
	mi := &file_llm_proto_msgTypes[9]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *StreamLLMRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*StreamLLMRequest) ProtoMessage() {}

func (x *StreamLLMRequest) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[9]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use StreamLLMRequest.ProtoReflect.Descriptor instead.
func (*StreamLLMRequest) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{9}
}

func (x *StreamLLMRequest) GetPayload() isStreamLLMRequest_Payload {
	if x != nil {
		return x.Payload
	}
	return nil
}

func (x *StreamLLMRequest) GetRequest() *LLMRequest {
	if x != nil {
		if x, ok := x.Payload.(*StreamLLMRequest_Request); ok {
			return x.Request
		}
	}
	return nil
}

func (x *StreamLLMRequest) GetHeartbeat() *emptypb.Empty {
	if x != nil {
		if x, ok := x.Payload.(*StreamLLMRequest_Heartbeat); ok {
			return x.Heartbeat
		}
	}
	return nil
}

type isStreamLLMRequest_Payload interface {
	isStreamLLMRequest_Payload()
}

type StreamLLMRequest_Request struct {
	Request *LLMRequest `protobuf:"bytes,1,opt,name=request,proto3,oneof"` // Initial request
}

type StreamLLMRequest_Heartbeat struct {
	Heartbeat *emptypb.Empty `protobuf:"bytes,2,opt,name=heartbeat,proto3,oneof"` // Keep-alive
}

func (*StreamLLMRequest_Request) isStreamLLMRequest_Payload() {}

func (*StreamLLMRequest_Heartbeat) isStreamLLMRequest_Payload() {}

// StreamLLMResponse is a response in the streaming LLM flow
type StreamLLMResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Types that are valid to be assigned to Payload:
	//
	//	*StreamLLMResponse_Chunk
	//	*StreamLLMResponse_ToolCall
	//	*StreamLLMResponse_Usage
	//	*StreamLLMResponse_Error
	//	*StreamLLMResponse_Complete
	//	*StreamLLMResponse_Heartbeat
	Payload       isStreamLLMResponse_Payload `protobuf_oneof:"payload"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *StreamLLMResponse) Reset() {
	*x = StreamLLMResponse{}
	mi := &file_llm_proto_msgTypes[10]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *StreamLLMResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*StreamLLMResponse) ProtoMessage() {}

func (x *StreamLLMResponse) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[10]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use StreamLLMResponse.ProtoReflect.Descriptor instead.
func (*StreamLLMResponse) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{10}
}

func (x *StreamLLMResponse) GetPayload() isStreamLLMResponse_Payload {
	if x != nil {
		return x.Payload
	}
	return nil
}

func (x *StreamLLMResponse) GetChunk() *StreamChunk {
	if x != nil {
		if x, ok := x.Payload.(*StreamLLMResponse_Chunk); ok {
			return x.Chunk
		}
	}
	return nil
}

func (x *StreamLLMResponse) GetToolCall() *StreamToolCall {
	if x != nil {
		if x, ok := x.Payload.(*StreamLLMResponse_ToolCall); ok {
			return x.ToolCall
		}
	}
	return nil
}

func (x *StreamLLMResponse) GetUsage() *StreamUsage {
	if x != nil {
		if x, ok := x.Payload.(*StreamLLMResponse_Usage); ok {
			return x.Usage
		}
	}
	return nil
}

func (x *StreamLLMResponse) GetError() *StreamError {
	if x != nil {
		if x, ok := x.Payload.(*StreamLLMResponse_Error); ok {
			return x.Error
		}
	}
	return nil
}

func (x *StreamLLMResponse) GetComplete() *StreamComplete {
	if x != nil {
		if x, ok := x.Payload.(*StreamLLMResponse_Complete); ok {
			return x.Complete
		}
	}
	return nil
}

func (x *StreamLLMResponse) GetHeartbeat() *emptypb.Empty {
	if x != nil {
		if x, ok := x.Payload.(*StreamLLMResponse_Heartbeat); ok {
			return x.Heartbeat
		}
	}
	return nil
}

type isStreamLLMResponse_Payload interface {
	isStreamLLMResponse_Payload()
}

type StreamLLMResponse_Chunk struct {
	Chunk *StreamChunk `protobuf:"bytes,1,opt,name=chunk,proto3,oneof"` // Response content chunk
}

type StreamLLMResponse_ToolCall struct {
	ToolCall *StreamToolCall `protobuf:"bytes,2,opt,name=tool_call,json=toolCall,proto3,oneof"` // Tool call during streaming
}

type StreamLLMResponse_Usage struct {
	Usage *StreamUsage `protobuf:"bytes,3,opt,name=usage,proto3,oneof"` // Token usage update
}

type StreamLLMResponse_Error struct {
	Error *StreamError `protobuf:"bytes,4,opt,name=error,proto3,oneof"` // Error during streaming
}

type StreamLLMResponse_Complete struct {
	Complete *StreamComplete `protobuf:"bytes,5,opt,name=complete,proto3,oneof"` // Stream completion
}

type StreamLLMResponse_Heartbeat struct {
	Heartbeat *emptypb.Empty `protobuf:"bytes,6,opt,name=heartbeat,proto3,oneof"` // Keep-alive
}

func (*StreamLLMResponse_Chunk) isStreamLLMResponse_Payload() {}

func (*StreamLLMResponse_ToolCall) isStreamLLMResponse_Payload() {}

func (*StreamLLMResponse_Usage) isStreamLLMResponse_Payload() {}

func (*StreamLLMResponse_Error) isStreamLLMResponse_Payload() {}

func (*StreamLLMResponse_Complete) isStreamLLMResponse_Payload() {}

func (*StreamLLMResponse_Heartbeat) isStreamLLMResponse_Payload() {}

// StreamChunk represents a content chunk in streaming response
type StreamChunk struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	RequestId     string                 `protobuf:"bytes,1,opt,name=request_id,json=requestId,proto3" json:"request_id,omitempty"`
	Content       string                 `protobuf:"bytes,2,opt,name=content,proto3" json:"content,omitempty"`              // Content delta
	Index         int32                  `protobuf:"varint,3,opt,name=index,proto3" json:"index,omitempty"`                 // Chunk index (for ordering)
	IsLast        bool                   `protobuf:"varint,4,opt,name=is_last,json=isLast,proto3" json:"is_last,omitempty"` // Whether this is the last content chunk
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *StreamChunk) Reset() {
	*x = StreamChunk{}
	mi := &file_llm_proto_msgTypes[11]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *StreamChunk) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*StreamChunk) ProtoMessage() {}

func (x *StreamChunk) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[11]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use StreamChunk.ProtoReflect.Descriptor instead.
func (*StreamChunk) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{11}
}

func (x *StreamChunk) GetRequestId() string {
	if x != nil {
		return x.RequestId
	}
	return ""
}

func (x *StreamChunk) GetContent() string {
	if x != nil {
		return x.Content
	}
	return ""
}

func (x *StreamChunk) GetIndex() int32 {
	if x != nil {
		return x.Index
	}
	return 0
}

func (x *StreamChunk) GetIsLast() bool {
	if x != nil {
		return x.IsLast
	}
	return false
}

// StreamToolCall represents a tool call during streaming
type StreamToolCall struct {
	state          protoimpl.MessageState `protogen:"open.v1"`
	RequestId      string                 `protobuf:"bytes,1,opt,name=request_id,json=requestId,proto3" json:"request_id,omitempty"`
	ToolCallId     string                 `protobuf:"bytes,2,opt,name=tool_call_id,json=toolCallId,proto3" json:"tool_call_id,omitempty"`
	Name           string                 `protobuf:"bytes,3,opt,name=name,proto3" json:"name,omitempty"`
	ArgumentsDelta string                 `protobuf:"bytes,4,opt,name=arguments_delta,json=argumentsDelta,proto3" json:"arguments_delta,omitempty"` // Partial arguments (JSON string)
	unknownFields  protoimpl.UnknownFields
	sizeCache      protoimpl.SizeCache
}

func (x *StreamToolCall) Reset() {
	*x = StreamToolCall{}
	mi := &file_llm_proto_msgTypes[12]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *StreamToolCall) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*StreamToolCall) ProtoMessage() {}

func (x *StreamToolCall) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[12]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use StreamToolCall.ProtoReflect.Descriptor instead.
func (*StreamToolCall) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{12}
}

func (x *StreamToolCall) GetRequestId() string {
	if x != nil {
		return x.RequestId
	}
	return ""
}

func (x *StreamToolCall) GetToolCallId() string {
	if x != nil {
		return x.ToolCallId
	}
	return ""
}

func (x *StreamToolCall) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *StreamToolCall) GetArgumentsDelta() string {
	if x != nil {
		return x.ArgumentsDelta
	}
	return ""
}

// StreamUsage represents token usage during streaming
type StreamUsage struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	RequestId     string                 `protobuf:"bytes,1,opt,name=request_id,json=requestId,proto3" json:"request_id,omitempty"`
	Usage         *TokenUsage            `protobuf:"bytes,2,opt,name=usage,proto3" json:"usage,omitempty"` // Current usage totals
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *StreamUsage) Reset() {
	*x = StreamUsage{}
	mi := &file_llm_proto_msgTypes[13]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *StreamUsage) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*StreamUsage) ProtoMessage() {}

func (x *StreamUsage) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[13]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use StreamUsage.ProtoReflect.Descriptor instead.
func (*StreamUsage) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{13}
}

func (x *StreamUsage) GetRequestId() string {
	if x != nil {
		return x.RequestId
	}
	return ""
}

func (x *StreamUsage) GetUsage() *TokenUsage {
	if x != nil {
		return x.Usage
	}
	return nil
}

// StreamError represents an error during streaming
type StreamError struct {
	state             protoimpl.MessageState `protogen:"open.v1"`
	RequestId         string                 `protobuf:"bytes,1,opt,name=request_id,json=requestId,proto3" json:"request_id,omitempty"`
	Code              string                 `protobuf:"bytes,2,opt,name=code,proto3" json:"code,omitempty"`                                                    // Error code
	Message           string                 `protobuf:"bytes,3,opt,name=message,proto3" json:"message,omitempty"`                                              // Error message
	Retryable         bool                   `protobuf:"varint,4,opt,name=retryable,proto3" json:"retryable,omitempty"`                                         // Whether the request can be retried
	SuggestedProvider string                 `protobuf:"bytes,5,opt,name=suggested_provider,json=suggestedProvider,proto3" json:"suggested_provider,omitempty"` // Optional: Suggested fallback provider
	unknownFields     protoimpl.UnknownFields
	sizeCache         protoimpl.SizeCache
}

func (x *StreamError) Reset() {
	*x = StreamError{}
	mi := &file_llm_proto_msgTypes[14]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *StreamError) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*StreamError) ProtoMessage() {}

func (x *StreamError) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[14]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use StreamError.ProtoReflect.Descriptor instead.
func (*StreamError) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{14}
}

func (x *StreamError) GetRequestId() string {
	if x != nil {
		return x.RequestId
	}
	return ""
}

func (x *StreamError) GetCode() string {
	if x != nil {
		return x.Code
	}
	return ""
}

func (x *StreamError) GetMessage() string {
	if x != nil {
		return x.Message
	}
	return ""
}

func (x *StreamError) GetRetryable() bool {
	if x != nil {
		return x.Retryable
	}
	return false
}

func (x *StreamError) GetSuggestedProvider() string {
	if x != nil {
		return x.SuggestedProvider
	}
	return ""
}

// StreamComplete indicates the stream has completed
type StreamComplete struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	RequestId     string                 `protobuf:"bytes,1,opt,name=request_id,json=requestId,proto3" json:"request_id,omitempty"`
	Response      *LLMResponse           `protobuf:"bytes,2,opt,name=response,proto3" json:"response,omitempty"` // Final complete response
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *StreamComplete) Reset() {
	*x = StreamComplete{}
	mi := &file_llm_proto_msgTypes[15]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *StreamComplete) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*StreamComplete) ProtoMessage() {}

func (x *StreamComplete) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[15]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use StreamComplete.ProtoReflect.Descriptor instead.
func (*StreamComplete) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{15}
}

func (x *StreamComplete) GetRequestId() string {
	if x != nil {
		return x.RequestId
	}
	return ""
}

func (x *StreamComplete) GetResponse() *LLMResponse {
	if x != nil {
		return x.Response
	}
	return nil
}

// CompleteLLMRequest is a non-streaming LLM request
type CompleteLLMRequest struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Request       *LLMRequest            `protobuf:"bytes,1,opt,name=request,proto3" json:"request,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *CompleteLLMRequest) Reset() {
	*x = CompleteLLMRequest{}
	mi := &file_llm_proto_msgTypes[16]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *CompleteLLMRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*CompleteLLMRequest) ProtoMessage() {}

func (x *CompleteLLMRequest) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[16]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use CompleteLLMRequest.ProtoReflect.Descriptor instead.
func (*CompleteLLMRequest) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{16}
}

func (x *CompleteLLMRequest) GetRequest() *LLMRequest {
	if x != nil {
		return x.Request
	}
	return nil
}

// CompleteLLMResponse is a non-streaming LLM response
type CompleteLLMResponse struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Response      *LLMResponse           `protobuf:"bytes,1,opt,name=response,proto3" json:"response,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *CompleteLLMResponse) Reset() {
	*x = CompleteLLMResponse{}
	mi := &file_llm_proto_msgTypes[17]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *CompleteLLMResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*CompleteLLMResponse) ProtoMessage() {}

func (x *CompleteLLMResponse) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[17]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use CompleteLLMResponse.ProtoReflect.Descriptor instead.
func (*CompleteLLMResponse) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{17}
}

func (x *CompleteLLMResponse) GetResponse() *LLMResponse {
	if x != nil {
		return x.Response
	}
	return nil
}

// ListModelsRequest requests available models
type ListModelsRequest struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Provider      string                 `protobuf:"bytes,1,opt,name=provider,proto3" json:"provider,omitempty"` // Optional: Filter by provider
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ListModelsRequest) Reset() {
	*x = ListModelsRequest{}
	mi := &file_llm_proto_msgTypes[18]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ListModelsRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ListModelsRequest) ProtoMessage() {}

func (x *ListModelsRequest) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[18]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ListModelsRequest.ProtoReflect.Descriptor instead.
func (*ListModelsRequest) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{18}
}

func (x *ListModelsRequest) GetProvider() string {
	if x != nil {
		return x.Provider
	}
	return ""
}

// ListModelsResponse returns available models
type ListModelsResponse struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Models        []*ModelInfo           `protobuf:"bytes,1,rep,name=models,proto3" json:"models,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ListModelsResponse) Reset() {
	*x = ListModelsResponse{}
	mi := &file_llm_proto_msgTypes[19]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ListModelsResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ListModelsResponse) ProtoMessage() {}

func (x *ListModelsResponse) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[19]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ListModelsResponse.ProtoReflect.Descriptor instead.
func (*ListModelsResponse) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{19}
}

func (x *ListModelsResponse) GetModels() []*ModelInfo {
	if x != nil {
		return x.Models
	}
	return nil
}

// ModelInfo contains information about a model
type ModelInfo struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Id            string                 `protobuf:"bytes,1,opt,name=id,proto3" json:"id,omitempty"`                                                                                       // Model identifier
	Name          string                 `protobuf:"bytes,2,opt,name=name,proto3" json:"name,omitempty"`                                                                                   // Human-readable name
	Provider      string                 `protobuf:"bytes,3,opt,name=provider,proto3" json:"provider,omitempty"`                                                                           // Provider name
	Capabilities  *ModelCapabilities     `protobuf:"bytes,4,opt,name=capabilities,proto3" json:"capabilities,omitempty"`                                                                   // Model capabilities
	Metadata      map[string]string      `protobuf:"bytes,5,rep,name=metadata,proto3" json:"metadata,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"` // Additional model metadata
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ModelInfo) Reset() {
	*x = ModelInfo{}
	mi := &file_llm_proto_msgTypes[20]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ModelInfo) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ModelInfo) ProtoMessage() {}

func (x *ModelInfo) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[20]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ModelInfo.ProtoReflect.Descriptor instead.
func (*ModelInfo) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{20}
}

func (x *ModelInfo) GetId() string {
	if x != nil {
		return x.Id
	}
	return ""
}

func (x *ModelInfo) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *ModelInfo) GetProvider() string {
	if x != nil {
		return x.Provider
	}
	return ""
}

func (x *ModelInfo) GetCapabilities() *ModelCapabilities {
	if x != nil {
		return x.Capabilities
	}
	return nil
}

func (x *ModelInfo) GetMetadata() map[string]string {
	if x != nil {
		return x.Metadata
	}
	return nil
}

// GetCapabilitiesRequest requests provider capabilities
type GetCapabilitiesRequest struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Provider      string                 `protobuf:"bytes,1,opt,name=provider,proto3" json:"provider,omitempty"` // Optional: Specific provider
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GetCapabilitiesRequest) Reset() {
	*x = GetCapabilitiesRequest{}
	mi := &file_llm_proto_msgTypes[21]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GetCapabilitiesRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GetCapabilitiesRequest) ProtoMessage() {}

func (x *GetCapabilitiesRequest) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[21]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GetCapabilitiesRequest.ProtoReflect.Descriptor instead.
func (*GetCapabilitiesRequest) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{21}
}

func (x *GetCapabilitiesRequest) GetProvider() string {
	if x != nil {
		return x.Provider
	}
	return ""
}

// GetCapabilitiesResponse returns provider capabilities
type GetCapabilitiesResponse struct {
	state         protoimpl.MessageState  `protogen:"open.v1"`
	Providers     []*ProviderCapabilities `protobuf:"bytes,1,rep,name=providers,proto3" json:"providers,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GetCapabilitiesResponse) Reset() {
	*x = GetCapabilitiesResponse{}
	mi := &file_llm_proto_msgTypes[22]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GetCapabilitiesResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GetCapabilitiesResponse) ProtoMessage() {}

func (x *GetCapabilitiesResponse) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[22]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GetCapabilitiesResponse.ProtoReflect.Descriptor instead.
func (*GetCapabilitiesResponse) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{22}
}

func (x *GetCapabilitiesResponse) GetProviders() []*ProviderCapabilities {
	if x != nil {
		return x.Providers
	}
	return nil
}

// ProviderCapabilities describes a provider's capabilities
type ProviderCapabilities struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Name          string                 `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`                                                                                   // Provider name
	Available     bool                   `protobuf:"varint,2,opt,name=available,proto3" json:"available,omitempty"`                                                                        // Whether provider is configured
	Models        []*ModelInfo           `protobuf:"bytes,3,rep,name=models,proto3" json:"models,omitempty"`                                                                               // Available models
	Metadata      map[string]string      `protobuf:"bytes,4,rep,name=metadata,proto3" json:"metadata,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"` // Additional provider metadata
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ProviderCapabilities) Reset() {
	*x = ProviderCapabilities{}
	mi := &file_llm_proto_msgTypes[23]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ProviderCapabilities) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ProviderCapabilities) ProtoMessage() {}

func (x *ProviderCapabilities) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[23]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ProviderCapabilities.ProtoReflect.Descriptor instead.
func (*ProviderCapabilities) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{23}
}

func (x *ProviderCapabilities) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *ProviderCapabilities) GetAvailable() bool {
	if x != nil {
		return x.Available
	}
	return false
}

func (x *ProviderCapabilities) GetModels() []*ModelInfo {
	if x != nil {
		return x.Models
	}
	return nil
}

func (x *ProviderCapabilities) GetMetadata() map[string]string {
	if x != nil {
		return x.Metadata
	}
	return nil
}

// LLMHealthCheckResponse contains health check information
type LLMHealthCheckResponse struct {
	state                protoimpl.MessageState `protogen:"open.v1"`
	Health               Health                 `protobuf:"varint,1,opt,name=health,proto3,enum=common.v1.Health" json:"health,omitempty"`
	Version              string                 `protobuf:"bytes,2,opt,name=version,proto3" json:"version,omitempty"`
	AvailableProviders   []string               `protobuf:"bytes,3,rep,name=available_providers,json=availableProviders,proto3" json:"available_providers,omitempty"`
	UnavailableProviders []string               `protobuf:"bytes,4,rep,name=unavailable_providers,json=unavailableProviders,proto3" json:"unavailable_providers,omitempty"`
	Timestamp            *timestamppb.Timestamp `protobuf:"bytes,5,opt,name=timestamp,proto3" json:"timestamp,omitempty"`
	unknownFields        protoimpl.UnknownFields
	sizeCache            protoimpl.SizeCache
}

func (x *LLMHealthCheckResponse) Reset() {
	*x = LLMHealthCheckResponse{}
	mi := &file_llm_proto_msgTypes[24]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *LLMHealthCheckResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*LLMHealthCheckResponse) ProtoMessage() {}

func (x *LLMHealthCheckResponse) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[24]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use LLMHealthCheckResponse.ProtoReflect.Descriptor instead.
func (*LLMHealthCheckResponse) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{24}
}

func (x *LLMHealthCheckResponse) GetHealth() Health {
	if x != nil {
		return x.Health
	}
	return Health_HEALTH_UNSPECIFIED
}

func (x *LLMHealthCheckResponse) GetVersion() string {
	if x != nil {
		return x.Version
	}
	return ""
}

func (x *LLMHealthCheckResponse) GetAvailableProviders() []string {
	if x != nil {
		return x.AvailableProviders
	}
	return nil
}

func (x *LLMHealthCheckResponse) GetUnavailableProviders() []string {
	if x != nil {
		return x.UnavailableProviders
	}
	return nil
}

func (x *LLMHealthCheckResponse) GetTimestamp() *timestamppb.Timestamp {
	if x != nil {
		return x.Timestamp
	}
	return nil
}

// LLMStatusResponse contains status information
type LLMStatusResponse struct {
	state           protoimpl.MessageState     `protogen:"open.v1"`
	Status          Status                     `protobuf:"varint,1,opt,name=status,proto3,enum=common.v1.Status" json:"status,omitempty"`
	StartedAt       *timestamppb.Timestamp     `protobuf:"bytes,2,opt,name=started_at,json=startedAt,proto3" json:"started_at,omitempty"`
	Uptime          *durationpb.Duration       `protobuf:"bytes,3,opt,name=uptime,proto3" json:"uptime,omitempty"`
	ActiveRequests  int32                      `protobuf:"varint,4,opt,name=active_requests,json=activeRequests,proto3" json:"active_requests,omitempty"`
	TotalRequests   int64                      `protobuf:"varint,5,opt,name=total_requests,json=totalRequests,proto3" json:"total_requests,omitempty"`
	TotalTokensUsed int64                      `protobuf:"varint,6,opt,name=total_tokens_used,json=totalTokensUsed,proto3" json:"total_tokens_used,omitempty"` // Total tokens across all requests
	Providers       map[string]*ProviderStatus `protobuf:"bytes,7,rep,name=providers,proto3" json:"providers,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	unknownFields   protoimpl.UnknownFields
	sizeCache       protoimpl.SizeCache
}

func (x *LLMStatusResponse) Reset() {
	*x = LLMStatusResponse{}
	mi := &file_llm_proto_msgTypes[25]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *LLMStatusResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*LLMStatusResponse) ProtoMessage() {}

func (x *LLMStatusResponse) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[25]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use LLMStatusResponse.ProtoReflect.Descriptor instead.
func (*LLMStatusResponse) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{25}
}

func (x *LLMStatusResponse) GetStatus() Status {
	if x != nil {
		return x.Status
	}
	return Status_STATUS_UNSPECIFIED
}

func (x *LLMStatusResponse) GetStartedAt() *timestamppb.Timestamp {
	if x != nil {
		return x.StartedAt
	}
	return nil
}

func (x *LLMStatusResponse) GetUptime() *durationpb.Duration {
	if x != nil {
		return x.Uptime
	}
	return nil
}

func (x *LLMStatusResponse) GetActiveRequests() int32 {
	if x != nil {
		return x.ActiveRequests
	}
	return 0
}

func (x *LLMStatusResponse) GetTotalRequests() int64 {
	if x != nil {
		return x.TotalRequests
	}
	return 0
}

func (x *LLMStatusResponse) GetTotalTokensUsed() int64 {
	if x != nil {
		return x.TotalTokensUsed
	}
	return 0
}

func (x *LLMStatusResponse) GetProviders() map[string]*ProviderStatus {
	if x != nil {
		return x.Providers
	}
	return nil
}

// ProviderStatus contains status for a specific provider
type ProviderStatus struct {
	state             protoimpl.MessageState `protogen:"open.v1"`
	Available         bool                   `protobuf:"varint,1,opt,name=available,proto3" json:"available,omitempty"`
	Health            Health                 `protobuf:"varint,2,opt,name=health,proto3,enum=common.v1.Health" json:"health,omitempty"`
	ActiveRequests    int32                  `protobuf:"varint,3,opt,name=active_requests,json=activeRequests,proto3" json:"active_requests,omitempty"`
	TotalRequests     int64                  `protobuf:"varint,4,opt,name=total_requests,json=totalRequests,proto3" json:"total_requests,omitempty"`
	AvgResponseTimeMs float64                `protobuf:"fixed64,5,opt,name=avg_response_time_ms,json=avgResponseTimeMs,proto3" json:"avg_response_time_ms,omitempty"` // Average response time
	LastRequestAt     *timestamppb.Timestamp `protobuf:"bytes,6,opt,name=last_request_at,json=lastRequestAt,proto3" json:"last_request_at,omitempty"`
	unknownFields     protoimpl.UnknownFields
	sizeCache         protoimpl.SizeCache
}

func (x *ProviderStatus) Reset() {
	*x = ProviderStatus{}
	mi := &file_llm_proto_msgTypes[26]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ProviderStatus) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ProviderStatus) ProtoMessage() {}

func (x *ProviderStatus) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[26]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ProviderStatus.ProtoReflect.Descriptor instead.
func (*ProviderStatus) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{26}
}

func (x *ProviderStatus) GetAvailable() bool {
	if x != nil {
		return x.Available
	}
	return false
}

func (x *ProviderStatus) GetHealth() Health {
	if x != nil {
		return x.Health
	}
	return Health_HEALTH_UNSPECIFIED
}

func (x *ProviderStatus) GetActiveRequests() int32 {
	if x != nil {
		return x.ActiveRequests
	}
	return 0
}

func (x *ProviderStatus) GetTotalRequests() int64 {
	if x != nil {
		return x.TotalRequests
	}
	return 0
}

func (x *ProviderStatus) GetAvgResponseTimeMs() float64 {
	if x != nil {
		return x.AvgResponseTimeMs
	}
	return 0
}

func (x *ProviderStatus) GetLastRequestAt() *timestamppb.Timestamp {
	if x != nil {
		return x.LastRequestAt
	}
	return nil
}

var File_llm_proto protoreflect.FileDescriptor

const file_llm_proto_rawDesc = "" +
	"\n" +
	"\tllm.proto\x12\x06llm.v1\x1a\x1fgoogle/protobuf/timestamp.proto\x1a\x1egoogle/protobuf/duration.proto\x1a\x1bgoogle/protobuf/empty.proto\x1a\fcommon.proto\"\xe7\x02\n" +
	"\n" +
	"LLMRequest\x12\x1d\n" +
	"\n" +
	"request_id\x18\x01 \x01(\tR\trequestId\x12\x1d\n" +
	"\n" +
	"session_id\x18\x02 \x01(\tR\tsessionId\x12\x14\n" +
	"\x05model\x18\x03 \x01(\tR\x05model\x12.\n" +
	"\bmessages\x18\x04 \x03(\v2\x12.llm.v1.LLMMessageR\bmessages\x125\n" +
	"\n" +
	"parameters\x18\x05 \x01(\v2\x15.llm.v1.LLMParametersR\n" +
	"parameters\x12\"\n" +
	"\x05tools\x18\x06 \x03(\v2\f.llm.v1.ToolR\x05tools\x12\x1a\n" +
	"\bprovider\x18\a \x01(\tR\bprovider\x12-\n" +
	"\x12fallback_providers\x18\b \x03(\tR\x11fallbackProviders\x12/\n" +
	"\bmetadata\x18\t \x01(\v2\x13.llm.v1.LLMMetadataR\bmetadata\"\xa9\x01\n" +
	"\n" +
	"LLMMessage\x12\x12\n" +
	"\x04role\x18\x01 \x01(\tR\x04role\x12\x18\n" +
	"\acontent\x18\x02 \x01(\tR\acontent\x123\n" +
	"\x05extra\x18\x03 \x03(\v2\x1d.llm.v1.LLMMessage.ExtraEntryR\x05extra\x1a8\n" +
	"\n" +
	"ExtraEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\"\xb9\x01\n" +
	"\rLLMParameters\x12\x1d\n" +
	"\n" +
	"max_tokens\x18\x01 \x01(\x05R\tmaxTokens\x12 \n" +
	"\vtemperature\x18\x02 \x01(\x01R\vtemperature\x12\x13\n" +
	"\x05top_p\x18\x03 \x01(\x01R\x04topP\x12\x13\n" +
	"\x05top_k\x18\x04 \x01(\x05R\x04topK\x12%\n" +
	"\x0estop_sequences\x18\x05 \x03(\tR\rstopSequences\x12\x16\n" +
	"\x06stream\x18\x06 \x01(\bR\x06stream\"x\n" +
	"\x04Tool\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12 \n" +
	"\vdescription\x18\x02 \x01(\tR\vdescription\x12:\n" +
	"\finput_schema\x18\x03 \x01(\v2\x17.llm.v1.ToolInputSchemaR\vinputSchema\"\xdf\x01\n" +
	"\x0fToolInputSchema\x12\x12\n" +
	"\x04type\x18\x01 \x01(\tR\x04type\x12G\n" +
	"\n" +
	"properties\x18\x02 \x03(\v2'.llm.v1.ToolInputSchema.PropertiesEntryR\n" +
	"properties\x12\x1a\n" +
	"\brequired\x18\x03 \x03(\tR\brequired\x1aS\n" +
	"\x0fPropertiesEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12*\n" +
	"\x05value\x18\x02 \x01(\v2\x14.llm.v1.ToolPropertyR\x05value:\x028\x01\"X\n" +
	"\fToolProperty\x12\x12\n" +
	"\x04type\x18\x01 \x01(\tR\x04type\x12 \n" +
	"\vdescription\x18\x02 \x01(\tR\vdescription\x12\x12\n" +
	"\x04enum\x18\x03 \x03(\tR\x04enum\"\xc0\x02\n" +
	"\vLLMMetadata\x129\n" +
	"\n" +
	"created_at\x18\x01 \x01(\v2\x1a.google.protobuf.TimestampR\tcreatedAt\x12\x19\n" +
	"\bagent_id\x18\x02 \x01(\tR\aagentId\x12!\n" +
	"\fcontainer_id\x18\x03 \x01(\tR\vcontainerId\x127\n" +
	"\x06labels\x18\x04 \x03(\v2\x1f.llm.v1.LLMMetadata.LabelsEntryR\x06labels\x12\x1d\n" +
	"\n" +
	"timeout_ms\x18\x05 \x01(\x03R\ttimeoutMs\x12%\n" +
	"\x0ecorrelation_id\x18\x06 \x01(\tR\rcorrelationId\x1a9\n" +
	"\vLabelsEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\"\x81\x03\n" +
	"\vLLMResponse\x12\x1d\n" +
	"\n" +
	"request_id\x18\x01 \x01(\tR\trequestId\x12\x18\n" +
	"\acontent\x18\x02 \x01(\tR\acontent\x12/\n" +
	"\n" +
	"tool_calls\x18\x03 \x03(\v2\x10.llm.v1.ToolCallR\ttoolCalls\x12+\n" +
	"\x05usage\x18\x04 \x01(\v2\x15.common.v1.TokenUsageR\x05usage\x129\n" +
	"\rfinish_reason\x18\x05 \x01(\x0e2\x14.llm.v1.FinishReasonR\ffinishReason\x12\x1a\n" +
	"\bprovider\x18\x06 \x01(\tR\bprovider\x12\x14\n" +
	"\x05model\x18\a \x01(\tR\x05model\x12=\n" +
	"\fcompleted_at\x18\b \x01(\v2\x1a.google.protobuf.TimestampR\vcompletedAt\x12/\n" +
	"\bmetadata\x18\t \x01(\v2\x13.llm.v1.LLMMetadataR\bmetadata\"L\n" +
	"\bToolCall\x12\x0e\n" +
	"\x02id\x18\x01 \x01(\tR\x02id\x12\x12\n" +
	"\x04name\x18\x02 \x01(\tR\x04name\x12\x1c\n" +
	"\targuments\x18\x03 \x01(\tR\targuments\"\x85\x01\n" +
	"\x10StreamLLMRequest\x12.\n" +
	"\arequest\x18\x01 \x01(\v2\x12.llm.v1.LLMRequestH\x00R\arequest\x126\n" +
	"\theartbeat\x18\x02 \x01(\v2\x16.google.protobuf.EmptyH\x00R\theartbeatB\t\n" +
	"\apayload\"\xca\x02\n" +
	"\x11StreamLLMResponse\x12+\n" +
	"\x05chunk\x18\x01 \x01(\v2\x13.llm.v1.StreamChunkH\x00R\x05chunk\x125\n" +
	"\ttool_call\x18\x02 \x01(\v2\x16.llm.v1.StreamToolCallH\x00R\btoolCall\x12+\n" +
	"\x05usage\x18\x03 \x01(\v2\x13.llm.v1.StreamUsageH\x00R\x05usage\x12+\n" +
	"\x05error\x18\x04 \x01(\v2\x13.llm.v1.StreamErrorH\x00R\x05error\x124\n" +
	"\bcomplete\x18\x05 \x01(\v2\x16.llm.v1.StreamCompleteH\x00R\bcomplete\x126\n" +
	"\theartbeat\x18\x06 \x01(\v2\x16.google.protobuf.EmptyH\x00R\theartbeatB\t\n" +
	"\apayload\"u\n" +
	"\vStreamChunk\x12\x1d\n" +
	"\n" +
	"request_id\x18\x01 \x01(\tR\trequestId\x12\x18\n" +
	"\acontent\x18\x02 \x01(\tR\acontent\x12\x14\n" +
	"\x05index\x18\x03 \x01(\x05R\x05index\x12\x17\n" +
	"\ais_last\x18\x04 \x01(\bR\x06isLast\"\x8e\x01\n" +
	"\x0eStreamToolCall\x12\x1d\n" +
	"\n" +
	"request_id\x18\x01 \x01(\tR\trequestId\x12 \n" +
	"\ftool_call_id\x18\x02 \x01(\tR\n" +
	"toolCallId\x12\x12\n" +
	"\x04name\x18\x03 \x01(\tR\x04name\x12'\n" +
	"\x0farguments_delta\x18\x04 \x01(\tR\x0eargumentsDelta\"Y\n" +
	"\vStreamUsage\x12\x1d\n" +
	"\n" +
	"request_id\x18\x01 \x01(\tR\trequestId\x12+\n" +
	"\x05usage\x18\x02 \x01(\v2\x15.common.v1.TokenUsageR\x05usage\"\xa7\x01\n" +
	"\vStreamError\x12\x1d\n" +
	"\n" +
	"request_id\x18\x01 \x01(\tR\trequestId\x12\x12\n" +
	"\x04code\x18\x02 \x01(\tR\x04code\x12\x18\n" +
	"\amessage\x18\x03 \x01(\tR\amessage\x12\x1c\n" +
	"\tretryable\x18\x04 \x01(\bR\tretryable\x12-\n" +
	"\x12suggested_provider\x18\x05 \x01(\tR\x11suggestedProvider\"`\n" +
	"\x0eStreamComplete\x12\x1d\n" +
	"\n" +
	"request_id\x18\x01 \x01(\tR\trequestId\x12/\n" +
	"\bresponse\x18\x02 \x01(\v2\x13.llm.v1.LLMResponseR\bresponse\"B\n" +
	"\x12CompleteLLMRequest\x12,\n" +
	"\arequest\x18\x01 \x01(\v2\x12.llm.v1.LLMRequestR\arequest\"F\n" +
	"\x13CompleteLLMResponse\x12/\n" +
	"\bresponse\x18\x01 \x01(\v2\x13.llm.v1.LLMResponseR\bresponse\"/\n" +
	"\x11ListModelsRequest\x12\x1a\n" +
	"\bprovider\x18\x01 \x01(\tR\bprovider\"?\n" +
	"\x12ListModelsResponse\x12)\n" +
	"\x06models\x18\x01 \x03(\v2\x11.llm.v1.ModelInfoR\x06models\"\x87\x02\n" +
	"\tModelInfo\x12\x0e\n" +
	"\x02id\x18\x01 \x01(\tR\x02id\x12\x12\n" +
	"\x04name\x18\x02 \x01(\tR\x04name\x12\x1a\n" +
	"\bprovider\x18\x03 \x01(\tR\bprovider\x12@\n" +
	"\fcapabilities\x18\x04 \x01(\v2\x1c.common.v1.ModelCapabilitiesR\fcapabilities\x12;\n" +
	"\bmetadata\x18\x05 \x03(\v2\x1f.llm.v1.ModelInfo.MetadataEntryR\bmetadata\x1a;\n" +
	"\rMetadataEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\"4\n" +
	"\x16GetCapabilitiesRequest\x12\x1a\n" +
	"\bprovider\x18\x01 \x01(\tR\bprovider\"U\n" +
	"\x17GetCapabilitiesResponse\x12:\n" +
	"\tproviders\x18\x01 \x03(\v2\x1c.llm.v1.ProviderCapabilitiesR\tproviders\"\xf8\x01\n" +
	"\x14ProviderCapabilities\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12\x1c\n" +
	"\tavailable\x18\x02 \x01(\bR\tavailable\x12)\n" +
	"\x06models\x18\x03 \x03(\v2\x11.llm.v1.ModelInfoR\x06models\x12F\n" +
	"\bmetadata\x18\x04 \x03(\v2*.llm.v1.ProviderCapabilities.MetadataEntryR\bmetadata\x1a;\n" +
	"\rMetadataEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\"\xfd\x01\n" +
	"\x16LLMHealthCheckResponse\x12)\n" +
	"\x06health\x18\x01 \x01(\x0e2\x11.common.v1.HealthR\x06health\x12\x18\n" +
	"\aversion\x18\x02 \x01(\tR\aversion\x12/\n" +
	"\x13available_providers\x18\x03 \x03(\tR\x12availableProviders\x123\n" +
	"\x15unavailable_providers\x18\x04 \x03(\tR\x14unavailableProviders\x128\n" +
	"\ttimestamp\x18\x05 \x01(\v2\x1a.google.protobuf.TimestampR\ttimestamp\"\xc6\x03\n" +
	"\x11LLMStatusResponse\x12)\n" +
	"\x06status\x18\x01 \x01(\x0e2\x11.common.v1.StatusR\x06status\x129\n" +
	"\n" +
	"started_at\x18\x02 \x01(\v2\x1a.google.protobuf.TimestampR\tstartedAt\x121\n" +
	"\x06uptime\x18\x03 \x01(\v2\x19.google.protobuf.DurationR\x06uptime\x12'\n" +
	"\x0factive_requests\x18\x04 \x01(\x05R\x0eactiveRequests\x12%\n" +
	"\x0etotal_requests\x18\x05 \x01(\x03R\rtotalRequests\x12*\n" +
	"\x11total_tokens_used\x18\x06 \x01(\x03R\x0ftotalTokensUsed\x12F\n" +
	"\tproviders\x18\a \x03(\v2(.llm.v1.LLMStatusResponse.ProvidersEntryR\tproviders\x1aT\n" +
	"\x0eProvidersEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12,\n" +
	"\x05value\x18\x02 \x01(\v2\x16.llm.v1.ProviderStatusR\x05value:\x028\x01\"\x9e\x02\n" +
	"\x0eProviderStatus\x12\x1c\n" +
	"\tavailable\x18\x01 \x01(\bR\tavailable\x12)\n" +
	"\x06health\x18\x02 \x01(\x0e2\x11.common.v1.HealthR\x06health\x12'\n" +
	"\x0factive_requests\x18\x03 \x01(\x05R\x0eactiveRequests\x12%\n" +
	"\x0etotal_requests\x18\x04 \x01(\x03R\rtotalRequests\x12/\n" +
	"\x14avg_response_time_ms\x18\x05 \x01(\x01R\x11avgResponseTimeMs\x12B\n" +
	"\x0flast_request_at\x18\x06 \x01(\v2\x1a.google.protobuf.TimestampR\rlastRequestAt*\xaf\x01\n" +
	"\fFinishReason\x12\x1d\n" +
	"\x19FINISH_REASON_UNSPECIFIED\x10\x00\x12\x16\n" +
	"\x12FINISH_REASON_STOP\x10\x01\x12\x18\n" +
	"\x14FINISH_REASON_LENGTH\x10\x02\x12\x1b\n" +
	"\x17FINISH_REASON_TOOL_USES\x10\x03\x12\x17\n" +
	"\x13FINISH_REASON_ERROR\x10\x04\x12\x18\n" +
	"\x14FINISH_REASON_FILTER\x10\x052\xba\x03\n" +
	"\n" +
	"LLMService\x12D\n" +
	"\tStreamLLM\x12\x18.llm.v1.StreamLLMRequest\x1a\x19.llm.v1.StreamLLMResponse(\x010\x01\x12F\n" +
	"\vCompleteLLM\x12\x1a.llm.v1.CompleteLLMRequest\x1a\x1b.llm.v1.CompleteLLMResponse\x12C\n" +
	"\n" +
	"ListModels\x12\x19.llm.v1.ListModelsRequest\x1a\x1a.llm.v1.ListModelsResponse\x12R\n" +
	"\x0fGetCapabilities\x12\x1e.llm.v1.GetCapabilitiesRequest\x1a\x1f.llm.v1.GetCapabilitiesResponse\x12E\n" +
	"\vHealthCheck\x12\x16.google.protobuf.Empty\x1a\x1e.llm.v1.LLMHealthCheckResponse\x12>\n" +
	"\tGetStatus\x12\x16.google.protobuf.Empty\x1a\x19.llm.v1.LLMStatusResponseB,Z*github.com/billm/baaaht/orchestrator/protob\x06proto3"

var (
	file_llm_proto_rawDescOnce sync.Once
	file_llm_proto_rawDescData []byte
)

func file_llm_proto_rawDescGZIP() []byte {
	file_llm_proto_rawDescOnce.Do(func() {
		file_llm_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_llm_proto_rawDesc), len(file_llm_proto_rawDesc)))
	})
	return file_llm_proto_rawDescData
}

var file_llm_proto_enumTypes = make([]protoimpl.EnumInfo, 1)
var file_llm_proto_msgTypes = make([]protoimpl.MessageInfo, 33)
var file_llm_proto_goTypes = []any{
	(FinishReason)(0),               // 0: llm.v1.FinishReason
	(*LLMRequest)(nil),              // 1: llm.v1.LLMRequest
	(*LLMMessage)(nil),              // 2: llm.v1.LLMMessage
	(*LLMParameters)(nil),           // 3: llm.v1.LLMParameters
	(*Tool)(nil),                    // 4: llm.v1.Tool
	(*ToolInputSchema)(nil),         // 5: llm.v1.ToolInputSchema
	(*ToolProperty)(nil),            // 6: llm.v1.ToolProperty
	(*LLMMetadata)(nil),             // 7: llm.v1.LLMMetadata
	(*LLMResponse)(nil),             // 8: llm.v1.LLMResponse
	(*ToolCall)(nil),                // 9: llm.v1.ToolCall
	(*StreamLLMRequest)(nil),        // 10: llm.v1.StreamLLMRequest
	(*StreamLLMResponse)(nil),       // 11: llm.v1.StreamLLMResponse
	(*StreamChunk)(nil),             // 12: llm.v1.StreamChunk
	(*StreamToolCall)(nil),          // 13: llm.v1.StreamToolCall
	(*StreamUsage)(nil),             // 14: llm.v1.StreamUsage
	(*StreamError)(nil),             // 15: llm.v1.StreamError
	(*StreamComplete)(nil),          // 16: llm.v1.StreamComplete
	(*CompleteLLMRequest)(nil),      // 17: llm.v1.CompleteLLMRequest
	(*CompleteLLMResponse)(nil),     // 18: llm.v1.CompleteLLMResponse
	(*ListModelsRequest)(nil),       // 19: llm.v1.ListModelsRequest
	(*ListModelsResponse)(nil),      // 20: llm.v1.ListModelsResponse
	(*ModelInfo)(nil),               // 21: llm.v1.ModelInfo
	(*GetCapabilitiesRequest)(nil),  // 22: llm.v1.GetCapabilitiesRequest
	(*GetCapabilitiesResponse)(nil), // 23: llm.v1.GetCapabilitiesResponse
	(*ProviderCapabilities)(nil),    // 24: llm.v1.ProviderCapabilities
	(*LLMHealthCheckResponse)(nil),  // 25: llm.v1.LLMHealthCheckResponse
	(*LLMStatusResponse)(nil),       // 26: llm.v1.LLMStatusResponse
	(*ProviderStatus)(nil),          // 27: llm.v1.ProviderStatus
	nil,                             // 28: llm.v1.LLMMessage.ExtraEntry
	nil,                             // 29: llm.v1.ToolInputSchema.PropertiesEntry
	nil,                             // 30: llm.v1.LLMMetadata.LabelsEntry
	nil,                             // 31: llm.v1.ModelInfo.MetadataEntry
	nil,                             // 32: llm.v1.ProviderCapabilities.MetadataEntry
	nil,                             // 33: llm.v1.LLMStatusResponse.ProvidersEntry
	(*timestamppb.Timestamp)(nil),   // 34: google.protobuf.Timestamp
	(*TokenUsage)(nil),              // 35: common.v1.TokenUsage
	(*emptypb.Empty)(nil),           // 36: google.protobuf.Empty
	(*ModelCapabilities)(nil),       // 37: common.v1.ModelCapabilities
	(Health)(0),                     // 38: common.v1.Health
	(Status)(0),                     // 39: common.v1.Status
	(*durationpb.Duration)(nil),     // 40: google.protobuf.Duration
}
var file_llm_proto_depIdxs = []int32{
	2,  // 0: llm.v1.LLMRequest.messages:type_name -> llm.v1.LLMMessage
	3,  // 1: llm.v1.LLMRequest.parameters:type_name -> llm.v1.LLMParameters
	4,  // 2: llm.v1.LLMRequest.tools:type_name -> llm.v1.Tool
	7,  // 3: llm.v1.LLMRequest.metadata:type_name -> llm.v1.LLMMetadata
	28, // 4: llm.v1.LLMMessage.extra:type_name -> llm.v1.LLMMessage.ExtraEntry
	5,  // 5: llm.v1.Tool.input_schema:type_name -> llm.v1.ToolInputSchema
	29, // 6: llm.v1.ToolInputSchema.properties:type_name -> llm.v1.ToolInputSchema.PropertiesEntry
	34, // 7: llm.v1.LLMMetadata.created_at:type_name -> google.protobuf.Timestamp
	30, // 8: llm.v1.LLMMetadata.labels:type_name -> llm.v1.LLMMetadata.LabelsEntry
	9,  // 9: llm.v1.LLMResponse.tool_calls:type_name -> llm.v1.ToolCall
	35, // 10: llm.v1.LLMResponse.usage:type_name -> common.v1.TokenUsage
	0,  // 11: llm.v1.LLMResponse.finish_reason:type_name -> llm.v1.FinishReason
	34, // 12: llm.v1.LLMResponse.completed_at:type_name -> google.protobuf.Timestamp
	7,  // 13: llm.v1.LLMResponse.metadata:type_name -> llm.v1.LLMMetadata
	1,  // 14: llm.v1.StreamLLMRequest.request:type_name -> llm.v1.LLMRequest
	36, // 15: llm.v1.StreamLLMRequest.heartbeat:type_name -> google.protobuf.Empty
	12, // 16: llm.v1.StreamLLMResponse.chunk:type_name -> llm.v1.StreamChunk
	13, // 17: llm.v1.StreamLLMResponse.tool_call:type_name -> llm.v1.StreamToolCall
	14, // 18: llm.v1.StreamLLMResponse.usage:type_name -> llm.v1.StreamUsage
	15, // 19: llm.v1.StreamLLMResponse.error:type_name -> llm.v1.StreamError
	16, // 20: llm.v1.StreamLLMResponse.complete:type_name -> llm.v1.StreamComplete
	36, // 21: llm.v1.StreamLLMResponse.heartbeat:type_name -> google.protobuf.Empty
	35, // 22: llm.v1.StreamUsage.usage:type_name -> common.v1.TokenUsage
	8,  // 23: llm.v1.StreamComplete.response:type_name -> llm.v1.LLMResponse
	1,  // 24: llm.v1.CompleteLLMRequest.request:type_name -> llm.v1.LLMRequest
	8,  // 25: llm.v1.CompleteLLMResponse.response:type_name -> llm.v1.LLMResponse
	21, // 26: llm.v1.ListModelsResponse.models:type_name -> llm.v1.ModelInfo
	37, // 27: llm.v1.ModelInfo.capabilities:type_name -> common.v1.ModelCapabilities
	31, // 28: llm.v1.ModelInfo.metadata:type_name -> llm.v1.ModelInfo.MetadataEntry
	24, // 29: llm.v1.GetCapabilitiesResponse.providers:type_name -> llm.v1.ProviderCapabilities
	21, // 30: llm.v1.ProviderCapabilities.models:type_name -> llm.v1.ModelInfo
	32, // 31: llm.v1.ProviderCapabilities.metadata:type_name -> llm.v1.ProviderCapabilities.MetadataEntry
	38, // 32: llm.v1.LLMHealthCheckResponse.health:type_name -> common.v1.Health
	34, // 33: llm.v1.LLMHealthCheckResponse.timestamp:type_name -> google.protobuf.Timestamp
	39, // 34: llm.v1.LLMStatusResponse.status:type_name -> common.v1.Status
	34, // 35: llm.v1.LLMStatusResponse.started_at:type_name -> google.protobuf.Timestamp
	40, // 36: llm.v1.LLMStatusResponse.uptime:type_name -> google.protobuf.Duration
	33, // 37: llm.v1.LLMStatusResponse.providers:type_name -> llm.v1.LLMStatusResponse.ProvidersEntry
	38, // 38: llm.v1.ProviderStatus.health:type_name -> common.v1.Health
	34, // 39: llm.v1.ProviderStatus.last_request_at:type_name -> google.protobuf.Timestamp
	6,  // 40: llm.v1.ToolInputSchema.PropertiesEntry.value:type_name -> llm.v1.ToolProperty
	27, // 41: llm.v1.LLMStatusResponse.ProvidersEntry.value:type_name -> llm.v1.ProviderStatus
	10, // 42: llm.v1.LLMService.StreamLLM:input_type -> llm.v1.StreamLLMRequest
	17, // 43: llm.v1.LLMService.CompleteLLM:input_type -> llm.v1.CompleteLLMRequest
	19, // 44: llm.v1.LLMService.ListModels:input_type -> llm.v1.ListModelsRequest
	22, // 45: llm.v1.LLMService.GetCapabilities:input_type -> llm.v1.GetCapabilitiesRequest
	36, // 46: llm.v1.LLMService.HealthCheck:input_type -> google.protobuf.Empty
	36, // 47: llm.v1.LLMService.GetStatus:input_type -> google.protobuf.Empty
	11, // 48: llm.v1.LLMService.StreamLLM:output_type -> llm.v1.StreamLLMResponse
	18, // 49: llm.v1.LLMService.CompleteLLM:output_type -> llm.v1.CompleteLLMResponse
	20, // 50: llm.v1.LLMService.ListModels:output_type -> llm.v1.ListModelsResponse
	23, // 51: llm.v1.LLMService.GetCapabilities:output_type -> llm.v1.GetCapabilitiesResponse
	25, // 52: llm.v1.LLMService.HealthCheck:output_type -> llm.v1.LLMHealthCheckResponse
	26, // 53: llm.v1.LLMService.GetStatus:output_type -> llm.v1.LLMStatusResponse
	48, // [48:54] is the sub-list for method output_type
	42, // [42:48] is the sub-list for method input_type
	42, // [42:42] is the sub-list for extension type_name
	42, // [42:42] is the sub-list for extension extendee
	0,  // [0:42] is the sub-list for field type_name
}

func init() { file_llm_proto_init() }
func file_llm_proto_init() {
	if File_llm_proto != nil {
		return
	}
	file_common_proto_init()
	file_llm_proto_msgTypes[9].OneofWrappers = []any{
		(*StreamLLMRequest_Request)(nil),
		(*StreamLLMRequest_Heartbeat)(nil),
	}
	file_llm_proto_msgTypes[10].OneofWrappers = []any{
		(*StreamLLMResponse_Chunk)(nil),
		(*StreamLLMResponse_ToolCall)(nil),
		(*StreamLLMResponse_Usage)(nil),
		(*StreamLLMResponse_Error)(nil),
		(*StreamLLMResponse_Complete)(nil),
		(*StreamLLMResponse_Heartbeat)(nil),
	}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_llm_proto_rawDesc), len(file_llm_proto_rawDesc)),
			NumEnums:      1,
			NumMessages:   33,
			NumExtensions: 0,
			NumServices:   1,
		},
		GoTypes:           file_llm_proto_goTypes,
		DependencyIndexes: file_llm_proto_depIdxs,
		EnumInfos:         file_llm_proto_enumTypes,
		MessageInfos:      file_llm_proto_msgTypes,
	}.Build()
	File_llm_proto = out.File
	file_llm_proto_goTypes = nil
	file_llm_proto_depIdxs = nil
}
